{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68206085",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Models 101</h1>\n",
    "\n",
    "Machine learning models are algorithms that can learn from data and make predictions or decisions on previously unseen data (stock prices, housing prices, etc.).\n",
    "\n",
    "In this tutorial, we are going to cover how machine learning models work. The lesson is broken into the following sections.\n",
    "\n",
    "<b>1. Data Preparation</b>: The data is cleaned, transformed, and divided into training and test sets.\n",
    "\n",
    "<b>2. Model Type Selection</b>: In Python we can create a variety of models (Decision Tree, Random Forest, etc.) using the scikit-learn module.\n",
    "\n",
    "<b>3. Model Training</b>: After we choose and create a model, we should fit/train the model, so it can capture and learn patterns from the training data.\n",
    "\n",
    "<b>4. Model Evaluation</b>: Once model training is complete, we can evaluate the model on the test set to measure its performance.\n",
    "\n",
    "Once a model is trained and evaluated, it can be used to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c7e6e",
   "metadata": {},
   "source": [
    "<h3>1. Data Preparation</h3>\n",
    "\n",
    "Data preparation for machine learning is the process of cleaning, transforming, and organizing the data so that it can be used to train a model. It is an important step as the quality of the data can greatly impact the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d5fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469a8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Funcitons & Tools\n",
    "\n",
    "# pd.DataFrame.head()\n",
    "# pd.DataFrame.describe()\n",
    "# pd.DataFrame.columns\n",
    "\n",
    "\n",
    "# Creating a new DataFrame containing only feature columns\n",
    "# features = ['col1', 'col3', 'col5', 'col7']\n",
    "# X = dataset[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196bbf7",
   "metadata": {},
   "source": [
    "<h2>Intro to Machine Learning At a Glance</h2>\n",
    "\n",
    "1. Read the training dataset into pandas DataFrame.\n",
    "2. Select target and features (X, y).\n",
    "3. Split the dataset into train and validation sets (X_train, X_val, y_train, y_val).\n",
    "4. Handle missing data (drop column, imputate both X_train and X_valid).\n",
    "5. Define, fit, predict, and evaluate models (we can use Mean Absolute Value to evaluate; we want lowest MAE). Choose the best model (create a function for this part).\n",
    "7. Re-fit the best model using the entire training dataset (X & y, instead of just X_train & y_train).\n",
    "8. Use the newly fitted model to predict the target with the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c07cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "419067f3",
   "metadata": {},
   "source": [
    "- fitting == training\n",
    "- evaluate == score\n",
    "- \"in-sample\" scores are a bad indication of model accuracy (we want to evaluate on previously unseen data to obtain accurate model accruacy). therefore, we must split the dataset into training vs. validation set.\n",
    "- Underfitting vs. Overfitting; underfitting refers to failure to capture relevant patterns, leading to less accurate predictions. overfitting refers to capturing spurious patterns that won't recur in the future, leading to less accruate predictions.\n",
    "- just like decision tree, we can adjust the parameters of a random forest model, but they generally work reasonably well even without this tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
